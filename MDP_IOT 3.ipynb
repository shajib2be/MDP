{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        #self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        #self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                160,\n",
    "                10,\n",
    "                 10,\n",
    "                10,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        \n",
    "        \n",
    "        \n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        \n",
    "\n",
    "        r=random.choices(t)\n",
    "        r[0][0], r[0][1], r[0][2], r[0][2] = self.state\n",
    "        r[0][1]=r[0][1]+1\n",
    "        self.state = (r[0][0], r[0][1], r[0][2], r[0][2])\n",
    "\n",
    "        done=bool(r[0][0]==1)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        \n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class CartPoleEnv1(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                1,\n",
    "                100,\n",
    "                100,\n",
    "                100,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        i=1\n",
    "         \n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        \n",
    "        #t=[1,5,4]\n",
    "        reward=0\n",
    "        a=5\n",
    "        b=5\n",
    "        \n",
    "        \n",
    "        #for i in range(20):\n",
    "        \n",
    "        if random.random()>math.exp(-a/b):\n",
    "            x=0\n",
    "            x_dot = 3\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            reward = -1\n",
    "            b+=1\n",
    "            a-=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            x = 1\n",
    "            x_dot =x_dot+i\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            \n",
    "            \n",
    "            \n",
    "            i+=1\n",
    "            reward = 1\n",
    "        #if self.kinematics_integrator == \"euler\":\n",
    "        #x = t[0][0]\n",
    "        \n",
    "          \n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(x_dot>40)\n",
    "        \n",
    "        a=a\n",
    "        b=b\n",
    "       \n",
    "        \n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #self.state = self.np_random.randint(low=0, high=1, size=(4,))\n",
    "        #self.state = (1,2,3,3),(1,2,3,3)\n",
    "      \n",
    "       self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "       self.steps_beyond_done = None\n",
    "       return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class CartPoleEnv3(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 1.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                1,\n",
    "                100,\n",
    "                100,\n",
    "                100,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        d=1\n",
    "         \n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        \n",
    "        #t=[1,5,4]\n",
    "        reward=0\n",
    "        a=35\n",
    "        b=50\n",
    "        k=0\n",
    "        \n",
    "        #for i in range(20):\n",
    "        \n",
    "        if  i==0:\n",
    "            \n",
    "            x = dot[0][k]\n",
    "            x_dot =x_dot+dot[1][k]+d\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            k+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            d+=1\n",
    "            reward = 1\n",
    "        \n",
    "        elif random.random()>math.exp(-a/b)and i!=0:\n",
    "            x = 1\n",
    "            d+=1*force\n",
    "            x_dot =x_dot+d\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            k+=1\n",
    "            d+=1*force\n",
    "            \n",
    "            \n",
    "            \n",
    "            reward = 1+d\n",
    "            \n",
    "            \n",
    "           \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x=0\n",
    "            x_dot = x_dot+d\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            reward = -(1+d)\n",
    "            \n",
    "            \n",
    "            b+=1\n",
    "            a-=1\n",
    "        #if self.kinematics_integrator == \"euler\":\n",
    "        #x = t[0][0]\n",
    "        \n",
    "          \n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(x_dot>100)\n",
    "        \n",
    "        a=a\n",
    "        b=b\n",
    "       \n",
    "        \n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #self.state = self.np_random.randint(low=0, high=1, size=(4,))\n",
    "       self.state = (1),(1),(1),(1)\n",
    "      \n",
    "       #self.state = self.np_random.uniform(low=0, high=0, size=(4,))\n",
    "       #self.state=self.np.random.uniform(size = (4,), low = 0, high = 1)\n",
    "       self.steps_beyond_done = None\n",
    "       return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CartPoleEnv2(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        #err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        #assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot ** 2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=0.0, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal\n",
    "\n",
    "\n",
    "class DuelingLinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, ALPHA, n_actions, name, input_dims, chkpt_dir='tmp/dqn'):\n",
    "        super(DuelingLinearDeepQNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.V = nn.Linear(128, 1)\n",
    "        self.A = nn.Linear(128, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=ALPHA)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_dqn')\n",
    "\n",
    "    def forward(self, state):\n",
    "        l1 = F.relu(self.fc1(state))\n",
    "        l2 = F.relu(self.fc2(l1))\n",
    "        V = self.V(l2)\n",
    "        A = self.A(l2)\n",
    "\n",
    "        return V, A\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, gamma, epsilon, alpha, n_actions, input_dims,\n",
    "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
    "                 replace=1000, chkpt_dir='tmp/dqn'):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.replace_target_cnt = replace\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims, n_actions)\n",
    "\n",
    "        self.q_eval = DuelingLinearDeepQNetwork(alpha, n_actions, input_dims=input_dims,\n",
    "                                   name='q_eval', chkpt_dir=chkpt_dir)\n",
    "        self.q_next = DuelingLinearDeepQNetwork(alpha, n_actions, input_dims=input_dims,\n",
    "                                   name='q_next', chkpt_dir=chkpt_dir)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            observation = observation[np.newaxis,:]\n",
    "            state = T.tensor(observation).to(self.q_eval.device)\n",
    "            _, advantage = self.q_eval.forward(state)\n",
    "            action = T.argmax(advantage).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.replace_target_cnt is not None and \\\n",
    "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                         if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        # using T.Tensor seems to reset datatype to float\n",
    "        # using T.tensor preserves source data type\n",
    "        state = T.tensor(state).to(self.q_eval.device)\n",
    "        new_state = T.tensor(new_state).to(self.q_eval.device)\n",
    "        action = T.tensor(action).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "\n",
    "        V_s, A_s = self.q_eval.forward(state)\n",
    "        V_s_, A_s_ = self.q_next.forward(new_state)\n",
    "\n",
    "        q_pred = T.add(V_s, (A_s - A_s.mean(dim=1, keepdim=True))).gather(1,\n",
    "                                              action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        q_next = T.add(V_s_, (A_s_ - A_s_.mean(dim=1, keepdim=True)))\n",
    "        q_target = rewards + self.gamma*T.max(q_next, dim=1)[0].detach()\n",
    "        q_target[dones] = 0.0\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def plotLearning(x, scores, epsilons,lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    #ax2.xaxis.tick_top()\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    #ax2.xaxis.set_label_position('top')\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c9a591809254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mload_checkpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     agent = Agent(gamma=0.99, epsilon=1.0, \n\u001b[0m\u001b[0;32m     12\u001b[0m                   \u001b[0minput_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                   batch_size=64, eps_dec=1e-3, replace=100)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'alpha'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "#from dueling_ddqn_torch import Agent\n",
    "#from utils import plotLearning\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = CartPoleEnv3()\n",
    "    num_games = 250\n",
    "    load_checkpoint = False\n",
    "\n",
    "    agent = Agent(gamma=0.99, epsilon=1.0, lr=5e-4,\n",
    "                  input_dims=[4], n_actions=2, mem_size=100000, eps_min=0.01,\n",
    "                  batch_size=64, eps_dec=1e-3, replace=100)\n",
    "\n",
    "    if load_checkpoint:\n",
    "        agent.load_models()\n",
    "\n",
    "    #filename = 'LunarLander-Dueling-DDQN-512-Adam-lr0005-replace100.png'\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "    n_steps = 0\n",
    "    x_dot=20\n",
    "    dot=[[1,20],[1,21],[1,22],[1,24],[1,25],[1,20],[1,21],[1,22],[1,24],[1,25],[1,20],[1,21],[1,22],[1,24],[1,25],[1,20],[1,21],[1,22],[1,24],[1,25]]\n",
    "    for i in range(num_games):\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action,\n",
    "                                    reward, observation_, int(done))\n",
    "            agent.learn()\n",
    "\n",
    "            observation = observation_\n",
    "\n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[max(0, i-100):(i+1)])\n",
    "        print('episode: ', i,'score %.1f ' % score,\n",
    "             ' average score %.1f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon)\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            agent.save_models()\n",
    "\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "    \n",
    "    plotLearning(x, scores, eps_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'eps_end'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a13a24a6490b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#env = gym.make('CartPole-v1')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCartPoleEnv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=2, eps_end=0.01,\n\u001b[0m\u001b[0;32m     11\u001b[0m                   input_dims=[4], lr=0.001)\n\u001b[0;32m     12\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'eps_end'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "#from simple_dqn_torch_2020 import Agent\n",
    "#from utils import plotLearning\n",
    "import numpy as np\n",
    "\n",
    "#t=[[0,5,4],[0,4,4],[0,3,4],[0,3,4],[0,3,4],[1,7,4],[1,8,4],[0,3,4],[0,8,4]]\n",
    "if __name__ == '__main__':\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env=CartPoleEnv3()\n",
    "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=2, eps_end=0.01,\n",
    "                  input_dims=[4], lr=0.001)\n",
    "    scores, eps_history = [], []\n",
    "    n_games =500\n",
    "    x_dot=20\n",
    "    dot=[[1,20],[1,21],[1,22],[1,24],[1,25],[1,20],[1,21],[1,22],[1,24],[1,25],[1,20],[1,21],[1,22],[1,24],[1,25],[1,20],[1,21],[1,22],[1,24],[1,25]]\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        #observation=env.step(action)\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, \n",
    "                                    observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "\n",
    "        print('episode ', i, 'score %.2f' % score,\n",
    "                'average score %.2f' % avg_score,\n",
    "                'epsilon %.2f' % agent.epsilon)\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    #filename = 'lunar_lander.png'\n",
    "    plotLearning(x, scores, eps_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "t=[[1,5,4],[1,4,4],[1,3,4],[1,3,4],[1,3,4],[1,7,4],[1,8,4],[1,3,4],[1,8,4]]\n",
    "for i in range(len(t)):\n",
    "  r=random.choices(t)\n",
    "  if random.random()<.5:\n",
    "        if r[0][0]==1 and random.random()<math.exp(-1):\n",
    "          print(r[0][1])\n",
    "  else:\n",
    "    print('wrong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=math.exp(-1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[[1,5,4],[1,4,4],[1,3,4],[1,3,4],[1,3,4],[1,7,4],[1,8,4],[1,3,4],[1,8,4]]\n",
    "for i in range(len(t)):\n",
    "    x=t[i][0]+5\n",
    "    t[i][0]=x\n",
    "    print(t[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[[1,5,4],[1,4,4],[1,3,4],[1,3,4],[1,3,4],[0,7,4],[1,8,4],[1,3,4],[1,8,4]]\n",
    "\n",
    "a=0\n",
    "for i in range(len(t)):\n",
    "  if t[i][0]==1:\n",
    "     a-=1\n",
    "    \n",
    "    \n",
    "print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " a=0\n",
    "         for i in range(len(t)):\n",
    "           if t[i][0]==1:\n",
    "              a+=1\n",
    "         b=9-a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((5,3), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3]*[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f MDP_IOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np_random.uniform(low=0, high=1, size=(4,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    " print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
