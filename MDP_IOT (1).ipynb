{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        #self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        #self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                160,\n",
    "                10,\n",
    "                 10,\n",
    "                10,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        \n",
    "        \n",
    "        \n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        \n",
    "\n",
    "        r=random.choices(t)\n",
    "        r[0][0], r[0][1], r[0][2], r[0][2] = self.state\n",
    "        r[0][1]=r[0][1]+1\n",
    "        self.state = (r[0][0], r[0][1], r[0][2], r[0][2])\n",
    "\n",
    "        done=bool(r[0][0]==1)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        \n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class CartPoleEnv1(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                1,\n",
    "                100,\n",
    "                100,\n",
    "                100,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        i=1\n",
    "         \n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        \n",
    "        #t=[1,5,4]\n",
    "        reward=0\n",
    "        a=7\n",
    "        b=3\n",
    "        \n",
    "        \n",
    "        #for i in range(20):\n",
    "        \n",
    "        if random.random()>math.exp(-a/b):\n",
    "            \n",
    "            x = 1\n",
    "            x_dot =x_dot+i\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            \n",
    "            \n",
    "            \n",
    "            i+=1\n",
    "            reward = 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            x=0\n",
    "            x_dot = 3\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            reward = -1\n",
    "            b+=1\n",
    "            a-=1\n",
    "        #if self.kinematics_integrator == \"euler\":\n",
    "        #x = t[0][0]\n",
    "        \n",
    "          \n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(x_dot>40)\n",
    "        \n",
    "        a=a\n",
    "        b=b\n",
    "       \n",
    "        \n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #self.state = self.np_random.randint(low=0, high=1, size=(4,))\n",
    "        #self.state = (1,2,3,3),(1,2,3,3)\n",
    "      \n",
    "       self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "       self.steps_beyond_done = None\n",
    "       return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class CartPoleEnv3(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 1.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                1,\n",
    "                100,\n",
    "                100,\n",
    "                100,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        d=1\n",
    "         \n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        \n",
    "        #t=[1,5,4]\n",
    "        reward=0\n",
    "        a=7\n",
    "        b=3\n",
    "        k=0\n",
    "        \n",
    "        #for i in range(20):\n",
    "        \n",
    "        if random.random()>math.exp(-a*d/b) and i==0:\n",
    "            \n",
    "            x = 1\n",
    "            x_dot =x_dot+dot[k]+d\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            k+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            d+=1\n",
    "            reward = 1\n",
    "        \n",
    "        elif random.random()>math.exp(-a/b):\n",
    "            \n",
    "            x = 1\n",
    "            x_dot =x_dot+d\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            k+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            d+=1*force\n",
    "            reward = (1+d)\n",
    "            \n",
    "        else:\n",
    "            x=0\n",
    "            x_dot = 3\n",
    "            theta = 3\n",
    "            theta_dot = 4\n",
    "            reward = -1\n",
    "            b+=1\n",
    "            a-=1\n",
    "        #if self.kinematics_integrator == \"euler\":\n",
    "        #x = t[0][0]\n",
    "        \n",
    "          \n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(x_dot>40)\n",
    "        \n",
    "        a=a\n",
    "        b=b\n",
    "       \n",
    "        \n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #self.state = self.np_random.randint(low=0, high=1, size=(4,))\n",
    "        #self.state = (1,2,3,3),(1,2,3,3)\n",
    "      \n",
    "       self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "       self.steps_beyond_done = None\n",
    "       return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CartPoleEnv2(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        #err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        #assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot ** 2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.iter_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions,\n",
    "                                   input_dims=input_dims,\n",
    "                                   fc1_dims=256, fc2_dims=256)\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                         dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = terminal\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(\n",
    "                self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        action_batch = self.action_memory[batch]\n",
    "        reward_batch = T.tensor(\n",
    "                self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(\n",
    "                self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma*T.max(q_next, dim=1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        self.iter_cntr += 1\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "            if self.epsilon > self.eps_min else self.eps_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def plotLearning(x, scores, epsilons,lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    #ax2.xaxis.tick_top()\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    #ax2.xaxis.set_label_position('top')\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score 2.00 average score 2.00 epsilon 1.00\n",
      "episode  1 score 183.00 average score 92.50 epsilon 0.98\n",
      "episode  2 score 127.00 average score 104.00 epsilon 0.94\n",
      "episode  3 score 978.00 average score 322.50 epsilon 0.67\n",
      "episode  4 score 1232.00 average score 504.40 epsilon 0.36\n",
      "episode  5 score 1690.00 average score 702.00 epsilon 0.01\n",
      "episode  6 score 861.00 average score 724.71 epsilon 0.01\n",
      "episode  7 score 1060.00 average score 766.62 epsilon 0.01\n",
      "episode  8 score 344.00 average score 719.67 epsilon 0.01\n",
      "episode  9 score 795.00 average score 727.20 epsilon 0.01\n",
      "episode  10 score 686.00 average score 723.45 epsilon 0.01\n",
      "episode  11 score 207.00 average score 680.42 epsilon 0.01\n",
      "episode  12 score 897.00 average score 697.08 epsilon 0.01\n",
      "episode  13 score 190.00 average score 660.86 epsilon 0.01\n",
      "episode  14 score 568.00 average score 654.67 epsilon 0.01\n",
      "episode  15 score 150.00 average score 623.12 epsilon 0.01\n",
      "episode  16 score 1926.00 average score 699.76 epsilon 0.01\n",
      "episode  17 score 3303.00 average score 844.39 epsilon 0.01\n",
      "episode  18 score 2223.00 average score 916.95 epsilon 0.01\n",
      "episode  19 score 422.00 average score 892.20 epsilon 0.01\n",
      "episode  20 score 698.00 average score 882.95 epsilon 0.01\n",
      "episode  21 score 1118.00 average score 893.64 epsilon 0.01\n",
      "episode  22 score 1700.00 average score 928.70 epsilon 0.01\n",
      "episode  23 score 1181.00 average score 939.21 epsilon 0.01\n",
      "episode  24 score 358.00 average score 915.96 epsilon 0.01\n",
      "episode  25 score 406.00 average score 896.35 epsilon 0.01\n",
      "episode  26 score 2139.00 average score 942.37 epsilon 0.01\n",
      "episode  27 score 349.00 average score 921.18 epsilon 0.01\n",
      "episode  28 score 2979.00 average score 992.14 epsilon 0.01\n",
      "episode  29 score 232.00 average score 966.80 epsilon 0.01\n",
      "episode  30 score 182.00 average score 941.48 epsilon 0.01\n",
      "episode  31 score 1865.00 average score 970.34 epsilon 0.01\n",
      "episode  32 score 1409.00 average score 983.64 epsilon 0.01\n",
      "episode  33 score 2570.00 average score 1030.29 epsilon 0.01\n",
      "episode  34 score 1753.00 average score 1050.94 epsilon 0.01\n",
      "episode  35 score 1649.00 average score 1067.56 epsilon 0.01\n",
      "episode  36 score 917.00 average score 1063.49 epsilon 0.01\n",
      "episode  37 score 1916.00 average score 1085.92 epsilon 0.01\n",
      "episode  38 score 289.00 average score 1065.49 epsilon 0.01\n",
      "episode  39 score 483.00 average score 1050.92 epsilon 0.01\n",
      "episode  40 score 1232.00 average score 1055.34 epsilon 0.01\n",
      "episode  41 score 1915.00 average score 1075.81 epsilon 0.01\n",
      "episode  42 score 1675.00 average score 1089.74 epsilon 0.01\n",
      "episode  43 score 757.00 average score 1082.18 epsilon 0.01\n",
      "episode  44 score 161.00 average score 1061.71 epsilon 0.01\n",
      "episode  45 score 529.00 average score 1050.13 epsilon 0.01\n",
      "episode  46 score 3351.00 average score 1099.09 epsilon 0.01\n",
      "episode  47 score 1011.00 average score 1097.25 epsilon 0.01\n",
      "episode  48 score 527.00 average score 1085.61 epsilon 0.01\n",
      "episode  49 score 829.00 average score 1080.48 epsilon 0.01\n",
      "episode  50 score 1923.00 average score 1097.00 epsilon 0.01\n",
      "episode  51 score 719.00 average score 1089.73 epsilon 0.01\n",
      "episode  52 score 1594.00 average score 1099.25 epsilon 0.01\n",
      "episode  53 score 134.00 average score 1081.37 epsilon 0.01\n",
      "episode  54 score 2338.00 average score 1104.22 epsilon 0.01\n",
      "episode  55 score 1094.00 average score 1104.04 epsilon 0.01\n",
      "episode  56 score 350.00 average score 1090.81 epsilon 0.01\n",
      "episode  57 score 526.00 average score 1081.07 epsilon 0.01\n",
      "episode  58 score 119.00 average score 1064.76 epsilon 0.01\n",
      "episode  59 score 2789.00 average score 1093.50 epsilon 0.01\n",
      "episode  60 score 3508.00 average score 1133.08 epsilon 0.01\n",
      "episode  61 score 481.00 average score 1122.56 epsilon 0.01\n",
      "episode  62 score 792.00 average score 1117.32 epsilon 0.01\n",
      "episode  63 score 156.00 average score 1102.30 epsilon 0.01\n",
      "episode  64 score 1354.00 average score 1106.17 epsilon 0.01\n",
      "episode  65 score 1195.00 average score 1107.52 epsilon 0.01\n",
      "episode  66 score 140.00 average score 1093.07 epsilon 0.01\n",
      "episode  67 score 1128.00 average score 1093.59 epsilon 0.01\n",
      "episode  68 score 1813.00 average score 1104.01 epsilon 0.01\n",
      "episode  69 score 433.00 average score 1094.43 epsilon 0.01\n",
      "episode  70 score 1170.00 average score 1095.49 epsilon 0.01\n",
      "episode  71 score 1397.00 average score 1099.68 epsilon 0.01\n",
      "episode  72 score 996.00 average score 1098.26 epsilon 0.01\n",
      "episode  73 score 121.00 average score 1085.05 epsilon 0.01\n",
      "episode  74 score 1724.00 average score 1093.57 epsilon 0.01\n",
      "episode  75 score 1275.00 average score 1095.96 epsilon 0.01\n",
      "episode  76 score 1196.00 average score 1097.26 epsilon 0.01\n",
      "episode  77 score 599.00 average score 1090.87 epsilon 0.01\n",
      "episode  78 score 1480.00 average score 1095.80 epsilon 0.01\n",
      "episode  79 score 905.00 average score 1093.41 epsilon 0.01\n",
      "episode  80 score 559.00 average score 1086.81 epsilon 0.01\n",
      "episode  81 score 969.00 average score 1085.38 epsilon 0.01\n",
      "episode  82 score 584.00 average score 1079.34 epsilon 0.01\n",
      "episode  83 score 810.00 average score 1076.13 epsilon 0.01\n",
      "episode  84 score 1590.00 average score 1082.18 epsilon 0.01\n",
      "episode  85 score 1128.00 average score 1082.71 epsilon 0.01\n",
      "episode  86 score 875.00 average score 1080.32 epsilon 0.01\n",
      "episode  87 score 2174.00 average score 1092.75 epsilon 0.01\n",
      "episode  88 score 704.00 average score 1088.38 epsilon 0.01\n",
      "episode  89 score 1544.00 average score 1093.44 epsilon 0.01\n",
      "episode  90 score 544.00 average score 1087.41 epsilon 0.01\n",
      "episode  91 score 1451.00 average score 1091.36 epsilon 0.01\n",
      "episode  92 score 782.00 average score 1088.03 epsilon 0.01\n",
      "episode  93 score 356.00 average score 1080.24 epsilon 0.01\n",
      "episode  94 score 1820.00 average score 1088.03 epsilon 0.01\n",
      "episode  95 score 502.00 average score 1081.93 epsilon 0.01\n",
      "episode  96 score 2886.00 average score 1100.53 epsilon 0.01\n",
      "episode  97 score 731.00 average score 1096.76 epsilon 0.01\n",
      "episode  98 score 417.00 average score 1089.89 epsilon 0.01\n",
      "episode  99 score 138.00 average score 1080.37 epsilon 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "#from simple_dqn_torch_2020 import Agent\n",
    "#from utils import plotLearning\n",
    "import numpy as np\n",
    "\n",
    "#t=[[0,5,4],[0,4,4],[0,3,4],[0,3,4],[0,3,4],[1,7,4],[1,8,4],[0,3,4],[0,8,4]]\n",
    "if __name__ == '__main__':\n",
    "    #env = gym.make('CartPole-v1')\n",
    "    env=CartPoleEnv3()\n",
    "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=2, eps_end=0.01,\n",
    "                  input_dims=[4], lr=0.001)\n",
    "    scores, eps_history = [], []\n",
    "    n_games =100\n",
    "    x_dot=20\n",
    "    dot=[20,21,22,24,25]\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        #observation=env.step(action)\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, \n",
    "                                    observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "\n",
    "        print('episode ', i, 'score %.2f' % score,\n",
    "                'average score %.2f' % avg_score,\n",
    "                'epsilon %.2f' % agent.epsilon)\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    #filename = 'lunar_lander.png'\n",
    "    plotLearning(x, scores, eps_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "t=[[1,5,4],[1,4,4],[1,3,4],[1,3,4],[1,3,4],[1,7,4],[1,8,4],[1,3,4],[1,8,4]]\n",
    "for i in range(len(t)):\n",
    "  r=random.choices(t)\n",
    "  if random.random()<.5:\n",
    "        if r[0][0]==1 and random.random()<math.exp(-1):\n",
    "          print(r[0][1])\n",
    "  else:\n",
    "    print('wrong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=math.exp(-1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[[1,5,4],[1,4,4],[1,3,4],[1,3,4],[1,3,4],[1,7,4],[1,8,4],[1,3,4],[1,8,4]]\n",
    "for i in range(len(t)):\n",
    "    x=t[i][0]+5\n",
    "    t[i][0]=x\n",
    "    print(t[i][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[[1,5,4],[1,4,4],[1,3,4],[1,3,4],[1,3,4],[0,7,4],[1,8,4],[1,3,4],[1,8,4]]\n",
    "\n",
    "a=0\n",
    "for i in range(len(t)):\n",
    "  if t[i][0]==1:\n",
    "     a-=1\n",
    "    \n",
    "    \n",
    "print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " a=0\n",
    "         for i in range(len(t)):\n",
    "           if t[i][0]==1:\n",
    "              a+=1\n",
    "         b=9-a\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((5,3), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3]*[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f MDP_IOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
